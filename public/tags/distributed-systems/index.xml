<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distributed-Systems on Saurav Shrivastav</title>
    <link>http://localhost:1313/tags/distributed-systems/</link>
    <description>Recent content in Distributed-Systems on Saurav Shrivastav</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Designing Data-Intensive Applications</title>
      <link>http://localhost:1313/papershelf/designing-data-intensive-applications/</link>
      <pubDate>Sat, 28 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/papershelf/designing-data-intensive-applications/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;This book provides a comprehensive guide to building reliable, scalable, and maintainable applications that handle large amounts of data. Kleppmann covers the fundamental principles behind distributed data systems and the trade-offs involved in their design.&lt;/p&gt;&#xA;&lt;h2 id=&#34;key-insights&#34;&gt;Key Insights&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data models and query languages&lt;/strong&gt;: Understanding the strengths and weaknesses of relational, document, and graph databases&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Storage and retrieval&lt;/strong&gt;: How databases store data on disk and in memory, including indexing strategies&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Distributed data challenges&lt;/strong&gt;: Replication, partitioning, and consistency in distributed systems&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Batch and stream processing&lt;/strong&gt;: Different approaches to processing large datasets&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;my-notes&#34;&gt;My Notes&lt;/h2&gt;&#xA;&lt;p&gt;This is one of the most practical books on distributed systems I&amp;rsquo;ve read. Kleppmann does an excellent job of explaining complex concepts without getting too theoretical. The book is particularly strong in:&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Google File System</title>
      <link>http://localhost:1313/papershelf/google-file-system/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/papershelf/google-file-system/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;GFS is a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and delivers high aggregate performance to a large number of clients.&lt;/p&gt;&#xA;&lt;h2 id=&#34;key-insights&#34;&gt;Key Insights&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Component failures are the norm&lt;/strong&gt;: Design assumes constant hardware failures&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Huge files by traditional standards&lt;/strong&gt;: Optimized for multi-GB files&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Append-heavy workloads&lt;/strong&gt;: Most files are mutated by appending new data&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Co-designing applications and file system&lt;/strong&gt;: Applications and file system API evolved together&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;my-notes&#34;&gt;My Notes&lt;/h2&gt;&#xA;&lt;p&gt;This paper was groundbreaking because it showed how to build a reliable distributed file system using commodity hardware. Key design decisions that made this work:&lt;/p&gt;</description>
    </item>
    <item>
      <title>MapReduce: Simplified Data Processing on Large Clusters</title>
      <link>http://localhost:1313/papershelf/mapreduce/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/papershelf/mapreduce/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;&#xA;&lt;p&gt;MapReduce is a programming model and implementation for processing and generating large datasets. Users specify a map function that processes key/value pairs and a reduce function that merges all intermediate values associated with the same intermediate key.&lt;/p&gt;&#xA;&lt;h2 id=&#34;key-insights&#34;&gt;Key Insights&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Simple programming model&lt;/strong&gt;: Developers only need to write map and reduce functions&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Automatic parallelization&lt;/strong&gt;: The framework handles distribution, fault tolerance, and load balancing&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Fault tolerance&lt;/strong&gt;: Handles machine failures transparently by re-executing tasks&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Locality optimization&lt;/strong&gt;: Moves computation to data rather than data to computation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;my-notes&#34;&gt;My Notes&lt;/h2&gt;&#xA;&lt;p&gt;This paper is remarkable for how it simplified distributed computing. Before MapReduce, writing distributed programs was extremely complex. Key innovations:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
